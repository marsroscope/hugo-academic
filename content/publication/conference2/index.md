---
title: 'Towards Building Condition-Based Cross-Modality Intention-Aware Human-AI Cooperation under VR Environment'

# Authors
# If you created a profile for a user (e.g. the default `admin` user), write the username (folder name) here
# and it will be replaced with their full name and linked to their profile.
authors:
  - Ziyao He
  - Shiyuan Li
  - Yunpeng Song
  - Zhongmin Cai

# Author notes (optional)
author_notes:
  - ''
  - ''
  - ''
  - ''

date: '2024-05-11T00:00:00Z'
doi: '10.1145/3613904.3642360'

# Schedule page publish date (NOT publication's date).
publishDate: '2024-05-11T00:00:00Z'

# Publication type.
# Legend: 0 = Uncategorized; 1 = Conference paper; 2 = Journal article;
# 3 = Preprint / Working Paper; 4 = Report; 5 = Book; 6 = Book section;
# 7 = Thesis; 8 = Patent
publication_types: ['1']

# Publication name and optional abbreviated publication name.
publication: In *Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems*
publication_short: In *ACM*

abstract: To address critical challenges in effectively identifying user intent and forming relevant information presentations and recommendations in VR environments, we propose an innovative condition-based multi-modal human-AI cooperation framework. It highlights the intent tuples (intent, condition, intent prompt, action prompt) and 2-Large-Language-Models (2-LLMs) architecture. This design, utilizes “condition” as the core to describe tasks, dynamically match user interactions with intentions, and empower generations of various tailored multi-modal AI responses. The architecture of 2-LLMs separates the roles of intent detection and action generation, decreasing the prompt length and helping with generating appropriate responses. We implemented a VR-based intelligent furniture purchasing system based on the proposed framework and conducted a three-phase comparative user study. The results conclusively demonstrate the system’s superiority in time efficiency and accuracy, intention conveyance improvements, effective product acquisitions, and user satisfaction and cooperation preference. Our framework provides a promising approach towards personalized and efficient user experiences in VR.

# Summary. An optional shortened abstract.
summary: The paper proposes a condition-based multi-modal human-AI cooperation framework for enhancing user intent identification and information presentation in VR. It utilizes intent tuples and a 2-Large-Language-Models (2-LLMs) architecture, improving prompt length and response generation. A VR furniture purchasing system based on this framework outperforms in user study phases, promising personalized VR experiences.

tags: 
- VR
- Human-AI Cooperation
- Chatbot

# Display this page in the Featured widget?
featured: false

# Custom links (uncomment lines below)
# links:
# - name: Custom Link
#   url: http://example.org

url_pdf: 'https://dl.acm.org/doi/pdf/10.1145/3613904.3642360'
url_code: ''
url_dataset: ''
url_poster: ''
url_project: ''
url_slides: 'https://1drv.ms/p/s!Ah6h9a-iT34Ksx5e0idiXN8SGyLR?e=zM9hro'
url_source: 'https://dl.acm.org/doi/pdf/10.1145/3613904.3642360'
url_video: ''

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
image:
  caption: 'Teaser Figure of the artcle'
  focal_point: ''
  preview_only: false

# Associated Projects (optional).
#   Associate this publication with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `internal-project` references `content/project/internal-project/index.md`.
#   Otherwise, set `projects: []`.
projects: 
- "content/project/example/index.md"

# Slides (optional).
#   Associate this publication with Markdown slides.
#   Simply enter your slide deck's filename without extension.
#   E.g. `slides: "example"` references `content/slides/example/index.md`.
#   Otherwise, set `slides: ""`.
slides: "https://1drv.ms/p/s!Ah6h9a-iT34Ksx5e0idiXN8SGyLR?e=zM9hro"
---

{{% callout note %}}
This work is available online after CHI'24 (2024-05-11).
{{% /callout %}}
